from transformers import AutoTokenizer, AutoModelForCausalLM

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2-large")
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2-large")

def generate_use_cases(industry):
    # Generate a prompt based on the industry
    prompt = f"Propose AI/ML use cases for a company in the {industry} industry."
    
    # Tokenize the input
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # Generate the output from the model
    output = model.generate(inputs["input_ids"], max_length=150, num_return_sequences=1)
    
    # Decode the generated text
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    # Return the use cases generated by the model
    return generated_text.strip()

# Example usage
industry = "Healthcare"
use_cases = generate_use_cases(industry)
print(f"Generated use cases for {industry}: \n{use_cases}")
